{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models\n",
    "\n",
    "We will now look at another major architecture called Seq2Seq which basically takes sequences as input and outputs another sequence. Where can we use this?\n",
    "\n",
    "We generally use this for machine translation. Given a set of words in a language, we find what will be the its translation in another language. We will be attempting to do this for translating English to Hindi. We will also look at some new metrics to gauge the \"correctness\" of our model.\n",
    "\n",
    "To read more about Seq2Seq : https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "\n",
    "We will start by importing all necessary libraries and defining directory paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\fastai\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "D:\\Anaconda\\envs\\fastai\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# For file handling...\n",
    "import pandas as pd\n",
    "import os,string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "\n",
    "#For dataset creation...\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "\n",
    "\n",
    "#For model building...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# For model training...\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm,tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(),\"data\",\"english_to_hindi.txt\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "This section will be geared towards building functions for:\n",
    "1. Reading the text file\n",
    "2. Dealing with missing values if any\n",
    "3. Tokenizing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN    0\n",
      "HI    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>HI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Help!</td>\n",
       "      <td>बचाओ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>उछलो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>कूदो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>छलांग.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>नमस्ते।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EN       HI\n",
       "0   Help!    बचाओ!\n",
       "1   Jump.    उछलो.\n",
       "2   Jump.    कूदो.\n",
       "3   Jump.   छलांग.\n",
       "4  Hello!  नमस्ते।"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readFile(path,chkNa=True):\n",
    "    \"\"\"\n",
    "        Load data from a text file. The file must have Lang1(delimiter)Lang2 in each row.\n",
    "        Eg : \"Hello Hallo\" or \"Hello Ola\" {here, delimiter was space} \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(path,header=None,sep=\"\\t\",names=[\"EN\",\"HI\"])\n",
    "        if chkNa:\n",
    "            print(df.isna().sum())\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{path} does not specify a text file.\")    \n",
    "    except OSError:\n",
    "        print(f\"{path} does not exist\")\n",
    "\n",
    "#checking to make sure...\n",
    "df = readFile(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the \"HI\" part seen above\n",
    "\n",
    "The above dataframe holds strings from UTF-8 character encoding. The strings in the \"HI\" column are all formed from the devnagiri script. Unicode is a larger character map which encompasses(or \"supports\", for the layman :) ) many scripts like Cyrillic ( Russian, Ukranian, etc.) and those accents in french as well as the Umlauts in German(the a,e,i,o,u with a \"snakebite\" on top). Dealing with these strings is fairly easy if you know how they are formed. A good place to understand this is:\n",
    "\n",
    "https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
    "\n",
    "http://www.utf-8.com/\n",
    "\n",
    "To help you wrap our heads around, all the strings are made of characters. However, All the characters are represented in memory as streams of bits. As we wanted to incorporate more languages, we increased the number of bits. To look at what is the integer representation of a character or vice-versa, we use following functions:\n",
    "\n",
    "ord(char) -> int\n",
    "\n",
    "chr(int) -> char\n",
    "\n",
    "Note that the two functions are inverses of each other i.e, ord(chr(someInt) = someInt\n",
    "\n",
    "https://stackoverflow.com/questions/38454521/how-to-print-character-using-its-unicode-value-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "2431\n"
     ]
    }
   ],
   "source": [
    "strin = df[\"HI\"][0]\n",
    "\n",
    "# The first letter in Devnagiri script...\n",
    "print(ord(\"ऀ\"))\n",
    "\n",
    "# The last letter in Devnagiri script...\n",
    "print(ord(\"ॿ\"))\n",
    "\n",
    "#The difference between these two should cover all the letters...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मुझे', 'टिकटें', 'कहाँ', 'से', 'लेनीं', 'होंगीं']\n",
      "['Where', 'should', 'I', 'pick', 'the', 'tickets', 'up']\n",
      "['तुम', 'आज', 'सुबह', 'यहाँ', 'क्यों', 'आए']\n",
      "['Why', 'did', 'you', 'come', 'here', 'this', 'morning']\n"
     ]
    }
   ],
   "source": [
    "pad = \" <PAD> \"\n",
    "sentBegin = \"<BEG> \"\n",
    "sentEnd = \" <END>\"\n",
    "unk = \"<UNK>\"\n",
    "\n",
    "def clean(txt):\n",
    "    unwanted = \"~|\\\\/_।.?,*@#$%^&(){}[]=+\\\"-'\"\n",
    "    for char in unwanted:\n",
    "        txt = txt.replace(char,' ')\n",
    "    return txt\n",
    "\n",
    "def tokenize(txt):\n",
    "    txt = clean(txt) \n",
    "    tokens = txt.split()\n",
    "    return tokens\n",
    "\n",
    "for i in range(2008,2010):\n",
    "    #print(df[\"HI\"][i])\n",
    "    print(tokenize(df[\"HI\"][i]))\n",
    "    print(tokenize(df[\"EN\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN\n",
      "HI\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset and Dataloaders\n",
    "\n",
    "This section will deal with generating our torch dataset and datloaders. Our dataset class will be:\n",
    "1. Taking a txt file path as input\n",
    "2. Reading the txt file\n",
    "3. Tokenizing the text data\n",
    "4. Creating vocabulary\n",
    "5. Creating charMaps and reverse charMaps\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngHinData(Dataset):    \n",
    "    def __init__(self,path,maxVocabSize=500):\n",
    "        \"\"\"\n",
    "            Read a text file from path and generate the input and target sequences\n",
    "            Also generate english and hindi vocabulary with a max size.\n",
    "            The most commonly occuring words are chosen.\n",
    "        \"\"\"\n",
    "        self.maxVocabSize = maxVocabSize\n",
    "        \n",
    "        df = readFile(path,chkNa=False)\n",
    "        self.df = self.tokenizeDf(df)\n",
    "        \n",
    "        self.replaceRareTokens(self.df)\n",
    "        self.findThatFucker()\n",
    "        self.df = self.removeHighUnk(self.df)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.sequences[i],self.targets[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    \n",
    "    def tokenizeDf(self,df):\n",
    "        df[\"ENTokenized\"] = df.EN.apply(tokenize)\n",
    "        df[\"HITokenized\"] = df.HI.apply(tokenize)\n",
    "        return df\n",
    "    \n",
    "    def replaceRareTokens(self,df):\n",
    "        commonInputs = self.mostFreqTokens(df.ENTokenized.tolist())\n",
    "        commonTargets = self.mostFreqTokens(df.HITokenized.tolist())\n",
    "        \n",
    "        df.loc[:, 'ENTokenized'] = df.ENTokenized.apply(\n",
    "            lambda tokens: [token if token in commonInputs \n",
    "                            else \"<UNK>\" for token in tokens]\n",
    "        )\n",
    "        df.loc[:, 'HITokenized'] = df.HITokenized.apply(\n",
    "            lambda tokens: [token if token in commonTargets\n",
    "                            else \"<UNK>\" for token in tokens]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def mostFreqTokens(self,sequence):\n",
    "        allTokens = [word for sent in sequence for word in sent]\n",
    "        common_tokens = set(list(zip(*Counter(allTokens).most_common(self.maxVocabSize - 4)))[0])\n",
    "        return common_tokens\n",
    "    \n",
    "    def removeHighUnk(self, df, threshold=0.8):\n",
    "        \"\"\"Remove sequences with mostly <UNK>.\"\"\"\n",
    "        calculate_ratio = (\n",
    "            lambda tokens: sum(1 for token in tokens if token != '<UNK>')/ len(tokens) > threshold\n",
    "        )\n",
    "        \n",
    "        df = df[df.ENTokenized.apply(calculate_ratio)]\n",
    "        df = df[df.HITokenized.apply(calculate_ratio)]\n",
    "        return df\n",
    "    \n",
    "        \n",
    "    def findThatFucker(self):\n",
    "        for i,val in enumerate(self.df.HITokenized.values):\n",
    "            if len(val)==0:\n",
    "                print(f\"Found a target fucker, index: {i}\")\n",
    "                print(f\"English: {self.df.EN[i]}\")\n",
    "                print(f\"Hindi: {self.df.HI[i]}\")\n",
    "        \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a target fucker, index: 15643\n",
      "English: Clear\n",
      "Hindi: ...\n",
      "Found a target fucker, index: 28850\n",
      "English: Open a file to load\n",
      "Hindi: ~\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-320-e0394f73ec32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEngHinData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-319-541c943eca3c>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, maxVocabSize)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplaceRareTokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindThatFucker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveHighUnk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-541c943eca3c>\u001b[0m in \u001b[0;36mremoveHighUnk\u001b[1;34m(self, df, threshold)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENTokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculate_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHITokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculate_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\fastai\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-541c943eca3c>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;34m\"\"\"Remove sequences with mostly <UNK>.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         calculate_ratio = (\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'<UNK>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         )\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "train_ds = EngHinData(DATA_PATH,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>HI</th>\n",
       "      <th>ENTokenized</th>\n",
       "      <th>HITokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29405</th>\n",
       "      <td>Fade Curve</td>\n",
       "      <td>धीमा होने का वक्र</td>\n",
       "      <td>[&lt;UNK&gt;, Curve]</td>\n",
       "      <td>[&lt;UNK&gt;, होने, का, वक्र]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29406</th>\n",
       "      <td>Fade To Volume</td>\n",
       "      <td>इस आवाज़ तक धीमा हों</td>\n",
       "      <td>[&lt;UNK&gt;, To, &lt;UNK&gt;]</td>\n",
       "      <td>[इस, आवाज़, तक, &lt;UNK&gt;, हों]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29407</th>\n",
       "      <td>Fade Time</td>\n",
       "      <td>धीमा होने का समय</td>\n",
       "      <td>[&lt;UNK&gt;, Time]</td>\n",
       "      <td>[&lt;UNK&gt;, होने, का, समय]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29408</th>\n",
       "      <td>Start Fade</td>\n",
       "      <td>धीमा करना प्रारंभ करें</td>\n",
       "      <td>[Start, &lt;UNK&gt;]</td>\n",
       "      <td>[&lt;UNK&gt;, करना, प्रारंभ, करें]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29409</th>\n",
       "      <td>Cannot find demultiplexer plugin for the given...</td>\n",
       "      <td>दिए गए मीडिया डाटा के लिए डीमल्टीप्लेक्सर प्लग...</td>\n",
       "      <td>[Cannot, &lt;UNK&gt;, &lt;UNK&gt;, plugin, for, the, given...</td>\n",
       "      <td>[दिए, गए, मीडिया, डाटा, के, लिए, &lt;UNK&gt;, प्लगइन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29410</th>\n",
       "      <td>Playback failed because no valid audio or vide...</td>\n",
       "      <td>प्लेबैक असफल चूंकि कोई वैध ऑडियो या वीडियो आउट...</td>\n",
       "      <td>[&lt;UNK&gt;, &lt;UNK&gt;, &lt;UNK&gt;, no, &lt;UNK&gt;, &lt;UNK&gt;, or, vi...</td>\n",
       "      <td>[&lt;UNK&gt;, असफल, &lt;UNK&gt;, कोई, &lt;UNK&gt;, ऑडियो, या, वी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29411</th>\n",
       "      <td>fade curve</td>\n",
       "      <td>फेड कर्व</td>\n",
       "      <td>[&lt;UNK&gt;, curve]</td>\n",
       "      <td>[&lt;UNK&gt;, &lt;UNK&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29412</th>\n",
       "      <td>current volume</td>\n",
       "      <td>मौजूदा आवाज़</td>\n",
       "      <td>[current, &lt;UNK&gt;]</td>\n",
       "      <td>[मौजूदा, आवाज़]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29413</th>\n",
       "      <td>volume to fade to</td>\n",
       "      <td>आवाज़ जहां तक धीमा होना है</td>\n",
       "      <td>[&lt;UNK&gt;, to, &lt;UNK&gt;, to]</td>\n",
       "      <td>[आवाज़, &lt;UNK&gt;, तक, &lt;UNK&gt;, &lt;UNK&gt;, है]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29414</th>\n",
       "      <td>fade time in milliseconds</td>\n",
       "      <td>धीमा होने का समय मिलिसेकण्डों में</td>\n",
       "      <td>[&lt;UNK&gt;, time, in, &lt;UNK&gt;]</td>\n",
       "      <td>[&lt;UNK&gt;, होने, का, समय, &lt;UNK&gt;, में]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      EN  \\\n",
       "29405                                         Fade Curve   \n",
       "29406                                     Fade To Volume   \n",
       "29407                                          Fade Time   \n",
       "29408                                         Start Fade   \n",
       "29409  Cannot find demultiplexer plugin for the given...   \n",
       "29410  Playback failed because no valid audio or vide...   \n",
       "29411                                         fade curve   \n",
       "29412                                     current volume   \n",
       "29413                                  volume to fade to   \n",
       "29414                          fade time in milliseconds   \n",
       "\n",
       "                                                      HI  \\\n",
       "29405                                  धीमा होने का वक्र   \n",
       "29406                               इस आवाज़ तक धीमा हों   \n",
       "29407                                   धीमा होने का समय   \n",
       "29408                             धीमा करना प्रारंभ करें   \n",
       "29409  दिए गए मीडिया डाटा के लिए डीमल्टीप्लेक्सर प्लग...   \n",
       "29410  प्लेबैक असफल चूंकि कोई वैध ऑडियो या वीडियो आउट...   \n",
       "29411                                           फेड कर्व   \n",
       "29412                                       मौजूदा आवाज़   \n",
       "29413                         आवाज़ जहां तक धीमा होना है   \n",
       "29414                  धीमा होने का समय मिलिसेकण्डों में   \n",
       "\n",
       "                                             ENTokenized  \\\n",
       "29405                                     [<UNK>, Curve]   \n",
       "29406                                 [<UNK>, To, <UNK>]   \n",
       "29407                                      [<UNK>, Time]   \n",
       "29408                                     [Start, <UNK>]   \n",
       "29409  [Cannot, <UNK>, <UNK>, plugin, for, the, given...   \n",
       "29410  [<UNK>, <UNK>, <UNK>, no, <UNK>, <UNK>, or, vi...   \n",
       "29411                                     [<UNK>, curve]   \n",
       "29412                                   [current, <UNK>]   \n",
       "29413                             [<UNK>, to, <UNK>, to]   \n",
       "29414                           [<UNK>, time, in, <UNK>]   \n",
       "\n",
       "                                             HITokenized  \n",
       "29405                            [<UNK>, होने, का, वक्र]  \n",
       "29406                        [इस, आवाज़, तक, <UNK>, हों]  \n",
       "29407                             [<UNK>, होने, का, समय]  \n",
       "29408                       [<UNK>, करना, प्रारंभ, करें]  \n",
       "29409  [दिए, गए, मीडिया, डाटा, के, लिए, <UNK>, प्लगइन...  \n",
       "29410  [<UNK>, असफल, <UNK>, कोई, <UNK>, ऑडियो, या, वी...  \n",
       "29411                                     [<UNK>, <UNK>]  \n",
       "29412                                    [मौजूदा, आवाज़]  \n",
       "29413               [आवाज़, <UNK>, तक, <UNK>, <UNK>, है]  \n",
       "29414                 [<UNK>, होने, का, समय, <UNK>, में]  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
