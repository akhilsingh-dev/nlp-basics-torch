{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models\n",
    "\n",
    "We will now look at another major architecture called Seq2Seq which basically takes sequences as input and outputs another sequence. Where can we use this?\n",
    "\n",
    "We generally use this for machine translation. Given a set of words in a language, we find what will be the its translation in another language. We will be attempting to do this for translating English to Hindi. We will also look at some new metrics to gauge the \"correctness\" of our model.\n",
    "\n",
    "To read more about Seq2Seq : https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "\n",
    "We will start by importing all necessary libraries and defining directory paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For file handling...\n",
    "import pandas as pd\n",
    "import os,string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "\n",
    "#For dataset creation...\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "\n",
    "\n",
    "#For model building...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# For model training...\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm,tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(),\"data\",\"english_to_hindi.txt\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "This section will be geared towards building functions for:\n",
    "1. Reading the text file\n",
    "2. Dealing with missing values if any\n",
    "3. Tokenizing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN    0\n",
      "HI    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>HI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Help!</td>\n",
       "      <td>बचाओ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>उछलो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>कूदो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>छलांग.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>नमस्ते।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EN       HI\n",
       "0   Help!    बचाओ!\n",
       "1   Jump.    उछलो.\n",
       "2   Jump.    कूदो.\n",
       "3   Jump.   छलांग.\n",
       "4  Hello!  नमस्ते।"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readFile(path,chkNa=True):\n",
    "    \"\"\"\n",
    "        Load data from a text file. The file must have Lang1(delimiter)Lang2 in each row.\n",
    "        Eg : \"Hello Hallo\" or \"Hello Ola\" {here, delimiter was space} \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(path,header=None,sep=\"\\t\",names=[\"EN\",\"HI\"])\n",
    "        if chkNa:\n",
    "            print(df.isna().sum())\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{path} does not specify a text file.\")    \n",
    "    except OSError:\n",
    "        print(f\"{path} does not exist\")\n",
    "\n",
    "#checking to make sure...\n",
    "df = readFile(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the \"HI\" part seen above\n",
    "\n",
    "The above dataframe holds strings from UTF-8 character encoding. The strings in the \"HI\" column are all formed from the devnagiri script. Unicode is a larger character map which encompasses(or \"supports\", for the layman :) ) many scripts like Cyrillic ( Russian, Ukranian, etc.) and those accents in french as well as the Umlauts in German(the a,e,i,o,u with a \"snakebite\" on top). Dealing with these strings is fairly easy if you know how they are formed. A good place to understand this is:\n",
    "\n",
    "https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
    "\n",
    "http://www.utf-8.com/\n",
    "\n",
    "To help you wrap our heads around, all the strings are made of characters. However, All the characters are represented in memory as streams of bits. As we wanted to incorporate more languages, we increased the number of bits. To look at what is the integer representation of a character or vice-versa, we use following functions:\n",
    "\n",
    "ord(char) -> int\n",
    "\n",
    "chr(int) -> char\n",
    "\n",
    "Note that the two functions are inverses of each other i.e, ord(chr(someInt) = someInt\n",
    "\n",
    "https://stackoverflow.com/questions/38454521/how-to-print-character-using-its-unicode-value-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "2431\n"
     ]
    }
   ],
   "source": [
    "strin = df[\"HI\"][0]\n",
    "\n",
    "# The first letter in Devnagiri script...\n",
    "print(ord(\"ऀ\"))\n",
    "\n",
    "# The last letter in Devnagiri script...\n",
    "print(ord(\"ॿ\"))\n",
    "\n",
    "#The difference between these two should cover all the letters...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मुझे', 'टिकटें', 'कहाँ', 'से', 'लेनीं', 'होंगीं']\n",
      "['Where', 'should', 'I', 'pick', 'the', 'tickets', 'up']\n",
      "['तुम', 'आज', 'सुबह', 'यहाँ', 'क्यों', 'आए']\n",
      "['Why', 'did', 'you', 'come', 'here', 'this', 'morning']\n"
     ]
    }
   ],
   "source": [
    "pad = \" <PAD> \"\n",
    "sentBegin = \"<BEG> \"\n",
    "sentEnd = \" <END>\"\n",
    "unk = \"<UNK>\"\n",
    "\n",
    "def clean(txt):\n",
    "    unwanted = \"~|\\\\/_।.?,*@#$%^&(){}[]=+\\\"-'\"\n",
    "    for char in unwanted:\n",
    "        txt = txt.replace(char,' ')\n",
    "    return txt\n",
    "\n",
    "def tokenize(txt):\n",
    "    txt = clean(txt) \n",
    "    tokens = txt.split()\n",
    "    return tokens\n",
    "\n",
    "for i in range(2008,2010):\n",
    "    #print(df[\"HI\"][i])\n",
    "    print(tokenize(df[\"HI\"][i]))\n",
    "    print(tokenize(df[\"EN\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN\n",
      "HI\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset and Dataloaders\n",
    "\n",
    "This section will deal with generating our torch dataset and datloaders. Our dataset class will be:\n",
    "1. Taking a txt file path as input\n",
    "2. Reading the txt file\n",
    "3. Tokenizing the text data\n",
    "4. Creating vocabulary\n",
    "5. Creating charMaps and reverse charMaps\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngHinData(Dataset):    \n",
    "    def __init__(self,path,maxVocabSize=500):\n",
    "        \"\"\"\n",
    "            Read a text file from path and generate the input and target sequences\n",
    "            Also generate english and hindi vocabulary with a max size.\n",
    "            The most commonly occuring words are chosen.\n",
    "        \"\"\"\n",
    "        self.maxVocabSize = maxVocabSize\n",
    "        \n",
    "        df = readFile(path,chkNa=False)\n",
    "        self.df = self.tokenizeDf(df)\n",
    "        \n",
    "        #Generate a vocabulary for both languages...\n",
    "        enVocab = self.mostFreqTokens(self.df.ENTokenized.tolist())\n",
    "        hiVocab = self.mostFreqTokens(self.df.HITokenized.tolist())\n",
    "        \n",
    "        #Replace rare tokens with \"<UNK>\"\n",
    "        self.replaceRareTokens(self.df)\n",
    "        #Impute zero length targets...\n",
    "        self.findZeroTargets()\n",
    "        #Remove all datarows with >20% unknowns...\n",
    "        self.df = self.removeHighUnk(self.df)\n",
    "        \n",
    "        # Create char maps and reverse char maps\n",
    "        self.enEncoder,self.enDecoder = self.generateMaps(enVocab,rev=True)\n",
    "        self.hiEncoder,self.hiDecoder = self.generateMaps(hiVocab,rev=True)\n",
    "        \n",
    "        # Add <BEG> and <END> to all tokens...\n",
    "        self.appendExtras(self.df)\n",
    "        \n",
    "        # change tokens to indices...\n",
    "        self.token2idx(self.df)\n",
    "        \n",
    "        # Drop all columns except num...\n",
    "        self.df.drop([\"level_0\",\"index\",\"EN\",\"HI\",\"ENTokenized\",\"HITokenized\"],axis=1,inplace=True)\n",
    "        self.df.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.df.ENNum[i],self.df.HINum[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    \n",
    "    def token2idx(self,df):\n",
    "        df[\"ENNum\"] = df.ENTokenized.apply(lambda tokenList: [self.enEncoder[token] for token in tokenList])\n",
    "        df[\"HINum\"] = df.HITokenized.apply(lambda tokenList: [self.hiEncoder[token] for token in tokenList])\n",
    "    \n",
    "    \n",
    "    def appender(self,tokenList):\n",
    "        tokenList.insert(0,\"<BEG>\")\n",
    "        tokenList.append(\"<END>\")\n",
    "        return tokenList\n",
    "    \n",
    "        \n",
    "    def appendExtras(self,df):\n",
    "        \"\"\"\n",
    "            Adds <BEG> and <END> at the start and end of each tokenList\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        df.ENTokenized.apply(self.appender)\n",
    "        df.HITokenized.apply(self.appender)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def generateMaps(self,vocab,rev=False):\n",
    "        \"\"\"\n",
    "            Generates a dictionary {char : idx}\n",
    "            If rev is set to True, a reverse map will also be generated {idx : char}\n",
    "        \"\"\"\n",
    "        extras = [\"<PAD>\",\"<BEG>\",\"<END>\",\"<UNK>\"]    \n",
    "        charMap = {char : idx for idx,char in enumerate(vocab)}\n",
    "        for extra in extras:\n",
    "            charMap[extra] = len(charMap)\n",
    "        \n",
    "        if not rev:\n",
    "            return charMap\n",
    "        else:\n",
    "            revCharMap = {idx : char for char,idx in charMap.items()}\n",
    "            return charMap,revCharMap \n",
    "        \n",
    "    \n",
    "    def tokenizeDf(self,df):\n",
    "        df[\"ENTokenized\"] = df.EN.apply(tokenize)\n",
    "        df[\"HITokenized\"] = df.HI.apply(tokenize)\n",
    "        return df\n",
    "    \n",
    "    def replaceRareTokens(self,df):\n",
    "        commonInputs = self.mostFreqTokens(df.ENTokenized.tolist())\n",
    "        commonTargets = self.mostFreqTokens(df.HITokenized.tolist())\n",
    "        \n",
    "        df.loc[:, 'ENTokenized'] = df.ENTokenized.apply(\n",
    "            lambda tokens: [token if token in commonInputs \n",
    "                            else \"<UNK>\" for token in tokens]\n",
    "        )\n",
    "        df.loc[:, 'HITokenized'] = df.HITokenized.apply(\n",
    "            lambda tokens: [token if token in commonTargets\n",
    "                            else \"<UNK>\" for token in tokens]\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def mostFreqTokens(self,sequence):\n",
    "        allTokens = [word for sent in sequence for word in sent]\n",
    "        common_tokens = set(list(zip(*Counter(allTokens).most_common(self.maxVocabSize - 4)))[0])\n",
    "        return common_tokens\n",
    "    \n",
    "    def removeHighUnk(self, df, threshold=0.8):\n",
    "        \"\"\"Remove sequences with mostly <UNK>.\"\"\"\n",
    "        calculate_ratio = (\n",
    "            lambda tokens: sum(1 for token in tokens if token != '<UNK>')/ len(tokens) > threshold\n",
    "        )\n",
    "        \n",
    "        df = df[df.ENTokenized.apply(calculate_ratio)]\n",
    "        df = df[df.HITokenized.apply(calculate_ratio)]\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "    \n",
    "        \n",
    "    def findZeroTargets(self):\n",
    "        badVals = []\n",
    "        for i,val in enumerate(self.df.HITokenized.values):\n",
    "            if len(val)==0:\n",
    "                badVals.append(i)\n",
    "        \n",
    "        print(f\"Found {len(badVals)} bad values...Imputing them...\")\n",
    "        self.df.drop(badVals,axis=0,inplace=True)\n",
    "        self.df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 bad values...Imputing them...\n"
     ]
    }
   ],
   "source": [
    "ds = EngHinData(DATA_PATH,3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "The class defined above does the following:\n",
    "1. It reads from the path specified text file\n",
    "2. It creates a pandas dataframe from it\n",
    "3. It tokenizes both the english and the hindi sentences.\n",
    "4. It found the most frequently occuring tokens.\n",
    "5. Using the most frequent tokens, it replaced other tokens as UNK since they wont be in our charMap.\n",
    "6. It removed the datarows which have more than 20% UNKnowns (by default, can be changed).\n",
    "7. It creates a charMap (or vocabulary) for both hindi and english languages\n",
    "8. And a reverse charMap which helps to decode back(for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ENNum</th>\n",
       "      <th>HINum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[2997, 2344, 2322, 1548, 2998]</td>\n",
       "      <td>[2997, 656, 1568, 300, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2997, 782, 1927, 2998]</td>\n",
       "      <td>[2997, 1831, 876, 1942, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[2997, 1326, 2998]</td>\n",
       "      <td>[2997, 1258, 303, 2825, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[2997, 2384, 2306, 2998]</td>\n",
       "      <td>[2997, 1470, 1045, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[2997, 2384, 2306, 2998]</td>\n",
       "      <td>[2997, 1470, 2525, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[2997, 2344, 2529, 2998]</td>\n",
       "      <td>[2997, 656, 1994, 1391, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[2997, 2344, 2529, 2998]</td>\n",
       "      <td>[2997, 656, 1994, 669, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[2997, 2344, 2322, 1561, 2998]</td>\n",
       "      <td>[2997, 1837, 2826, 1911, 1391, 2825, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[2997, 125, 489, 2998]</td>\n",
       "      <td>[2997, 2485, 556, 2199, 2998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[2997, 2863, 489, 2998]</td>\n",
       "      <td>[2997, 201, 275, 2998]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                           ENNum  \\\n",
       "0      0  [2997, 2344, 2322, 1548, 2998]   \n",
       "1      1         [2997, 782, 1927, 2998]   \n",
       "2      2              [2997, 1326, 2998]   \n",
       "3      3        [2997, 2384, 2306, 2998]   \n",
       "4      4        [2997, 2384, 2306, 2998]   \n",
       "5      5        [2997, 2344, 2529, 2998]   \n",
       "6      6        [2997, 2344, 2529, 2998]   \n",
       "7      7  [2997, 2344, 2322, 1561, 2998]   \n",
       "8      8          [2997, 125, 489, 2998]   \n",
       "9      9         [2997, 2863, 489, 2998]   \n",
       "\n",
       "                                        HINum  \n",
       "0                [2997, 656, 1568, 300, 2998]  \n",
       "1               [2997, 1831, 876, 1942, 2998]  \n",
       "2               [2997, 1258, 303, 2825, 2998]  \n",
       "3                    [2997, 1470, 1045, 2998]  \n",
       "4                    [2997, 1470, 2525, 2998]  \n",
       "5               [2997, 656, 1994, 1391, 2998]  \n",
       "6                [2997, 656, 1994, 669, 2998]  \n",
       "7  [2997, 1837, 2826, 1911, 1391, 2825, 2998]  \n",
       "8               [2997, 2485, 556, 2199, 2998]  \n",
       "9                      [2997, 201, 275, 2998]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15063, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.99 * len(ds))\n",
    "test_size = len(ds) - train_size\n",
    "train_ds, test_ds = torch.utils.data.random_split(ds, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collation function and dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def collate(batch):\n",
    "    \n",
    "    inputs = [torch.LongTensor(item[0]) for item in batch]\n",
    "    targets = [torch.LongTensor(item[1]) for item in batch]\n",
    "    \n",
    "    \n",
    "    # Pad sequencse so that they are all the same length (within one minibatch)\n",
    "    padded_inputs = pad_sequence(inputs, padding_value=ds.enEncoder[\"<PAD>\"], batch_first=True)\n",
    "    padded_targets = pad_sequence(targets, padding_value=ds.hiEncoder[\"<PAD>\"], batch_first=True)\n",
    "    \n",
    "    \n",
    "    # Sort by length for CUDA optimizations\n",
    "    lengths = torch.LongTensor([len(x) for x in inputs])\n",
    "    lengths, permutation = lengths.sort(dim=0, descending=True)\n",
    "\n",
    "    return padded_inputs[permutation].to(device), padded_targets[permutation].to(device), lengths.to(device)\n",
    "\n",
    "\n",
    "batchSize = 1024\n",
    "train_loader = DataLoader(train_ds, batch_size=batchSize, collate_fn=collate)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "The model will have 2 major parts namely the encoder and the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocabSize,embDims,hiddenSize,batchSize):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        # Copy in all the required sizes...\n",
    "        self.batchSize = batchSize\n",
    "        self.hiddenSize= hiddenSize\n",
    "        self.vocabSize = vocabSize\n",
    "        self.embDims = embDims\n",
    "        \n",
    "        \n",
    "        # Encoder architecture...\n",
    "        self.embedding = nn.Embedding(vocabSize,embDims)\n",
    "        self.gru = nn.GRU(self.embDims,self.hiddenSize,batch_first=True)\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs,lengths):\n",
    "        self.batchSize = inputs.size(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output,self.hidden = self.gru(x,self.initWghts())\n",
    "        \n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        \n",
    "        return output,self.hidden\n",
    "    \n",
    "    \n",
    "    def initWghts(self):\n",
    "        wghts = torch.empty(1,self.batchSize,self.hiddenSize)\n",
    "        return nn.init.kaiming_normal_(wghts).to('cuda')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocabSize,embDims,encoderSize,decoderSize,batchSize):\n",
    "        \n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.vocabSize = vocabSize\n",
    "        self.encoderSize = encoderSize\n",
    "        self.decoderSize = decoderSize\n",
    "        self.embDims = embDims\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocabSize,self.embDims)\n",
    "        self.gru = nn.GRU(self.embDims+self.encoderSize,\n",
    "                          self.decoderSize,\n",
    "                         batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.encoderSize,self.vocabSize)\n",
    "        \n",
    "        self.W1 = nn.Linear(self.encoderSize,self.decoderSize)\n",
    "        self.W2 = nn.Linear(self.encoderSize,self.decoderSize)\n",
    "        self.V = nn.Linear(self.encoderSize,1)\n",
    "        \n",
    "    def forward(self,targets,hidden,encoderOutput):\n",
    "        self.batchSize = inputs.size(0)\n",
    "        \n",
    "        encoderOutput = encoderOutput.permute(1,0,2)\n",
    "        hiddenTimeAxis = hidden.permute(1,0,2)\n",
    "        \n",
    "        score = torch.tanh(self.W1(encoderOutput)+self.W2(hiddenTimeAxis))\n",
    "        \n",
    "        attention = torch.softmax(self.V(score),dim=1)\n",
    "        \n",
    "        context = attention * encoderOutput\n",
    "        context = torch.sum(context,dim=1)\n",
    "        \n",
    "        x = self.embedding(targets)\n",
    "        x = torch.cat((context.unsqueeze(1),x),-1)\n",
    "        \n",
    "        output,state = self.gru(x,self.initWghts())\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x,state,attention\n",
    "    \n",
    "    def initWghts(self):\n",
    "        wghts = torch.empty(1,self.batchSize,self.decoderSize)\n",
    "        return nn.init.kaiming_normal_(wghts).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_func(actual,predicted):\n",
    "    \n",
    "    mask = actual.ge(1).float().to('cuda')\n",
    "    #print(predicted.size(),actual.size())\n",
    "    loss = criterion(predicted.squeeze(1),actual) * mask\n",
    "    \n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngToHinModel(nn.Module):\n",
    "    def __init__(self,inputVocabSize,targetVocabSize,\n",
    "                 hiddenSize,embDims,batchSize,\n",
    "                 targetsStart,targetsEnd):\n",
    "        \n",
    "        super(EngToHinModel,self).__init__()\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.targetsStart = targetsStart\n",
    "        self.targetsEnd = targetsEnd\n",
    "        \n",
    "        \n",
    "        self.encoder = Encoder(inputVocabSize,embDims,\n",
    "                               hiddenSize,batchSize).to('cuda')\n",
    "        \n",
    "        self.decoder = Decoder(targetVocabSize,embDims,\n",
    "                               hiddenSize,hiddenSize,batchSize).to('cuda')\n",
    "    \n",
    "    def predict(self,inputs,lengths):\n",
    "        self.batchSize= inputs.size(0)\n",
    "        \n",
    "        encoderOutput,encoderHidden = self.encoder(inputs.to('cuda'),lengths)\n",
    "        decoderHidden = encoderHidden\n",
    "        \n",
    "        decoderInput = torch.LongTensor([[self.targetsStart]] * self.batchSize)\n",
    "        \n",
    "        output = []\n",
    "        for _ in range(20):\n",
    "            pred,decoderHidden,_ = self.decoder(decoderInput.to('cuda'),\n",
    "                                             decoderHidden.to('cuda'),\n",
    "                                             encoderOutput.to('cuda'))\n",
    "            \n",
    "            prediction = torch.multinomial(F.softmax(pred,dim=1),1)\n",
    "            decoderInput = prediction\n",
    "            \n",
    "            prediction = prediction.item()\n",
    "            output.append(prediction)\n",
    "            \n",
    "            if prediction == self.targetsEnd:\n",
    "                return output\n",
    "        \n",
    "    def forward(self,inputs,targets,lengths):\n",
    "        self.batchSize = inputs.size(0)\n",
    "\n",
    "        encOut, encHidden = self.encoder(inputs.to('cuda'),lengths)\n",
    "\n",
    "        decHidden = encHidden\n",
    "\n",
    "        decIn = torch.LongTensor([[self.targetsStart]] * self.batchSize)\n",
    "\n",
    "\n",
    "        #teacher forcing...\n",
    "        loss=0\n",
    "        for ts in range(1,targets.size(1)):\n",
    "            preds,decHidden,_ = self.decoder(decIn.to('cuda'),\n",
    "                                           decHidden.to('cuda'),\n",
    "                                           encOut.to('cuda'))\n",
    "            decIn = targets[:,ts].unsqueeze(1)\n",
    "\n",
    "            loss += loss_func(targets[:,ts],preds)\n",
    "\n",
    "        return loss/targets.size(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = EngToHinModel(inputVocabSize=len(ds.enEncoder),\n",
    "                        targetVocabSize=len(ds.hiEncoder),\n",
    "                        hiddenSize=128,\n",
    "                        embDims=100,\n",
    "                        batchSize=batchSize,\n",
    "                        targetsStart=ds.hiEncoder[\"<BEG>\"],\n",
    "                        targetsEnd=ds.hiEncoder[\"<END>\"] \n",
    "                        ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2df73b2b294acf9891bde609fe81ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  1\ttrain_loss: 4.76e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26034cc65084fb9ac478f018871ca59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  2\ttrain_loss: 4.65e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc586a839ce4bbb862f2c24a6449066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  3\ttrain_loss: 4.57e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3eee49dad94c359cb95f43be8833b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  4\ttrain_loss: 4.49e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47b2f3bcc324f68a7246ddf68a8b32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  5\ttrain_loss: 4.41e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dda4f688b694ac399c47b18a2b141ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  6\ttrain_loss: 4.33e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f98195219d4a8e96a30ac6f2497db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  7\ttrain_loss: 4.25e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1803d92f55b04efa98c0327d685c6dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  8\ttrain_loss: 4.17e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ab7e74f02b4ad6adf0d39514df7ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch #  9\ttrain_loss: 4.08e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9ecac133de4e7381fa2bdfd51a34e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 10\ttrain_loss: 4.00e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20574acbfb0a4aef8d40ed7dbf10b7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 11\ttrain_loss: 3.92e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e0f2e05e04321bad42d849d206736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 12\ttrain_loss: 3.83e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9c1e50f2664ce287746ed1b47f4e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 13\ttrain_loss: 3.75e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc5bc7fd1ca411796683da20e0579de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 14\ttrain_loss: 3.66e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2174ee09994de99d735d8b6aacd6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 15\ttrain_loss: 3.58e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b06caeeb214dd6a68f924072055e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 16\ttrain_loss: 3.49e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ead856472954ec4b804a160dd3dca25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 17\ttrain_loss: 3.41e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b71b9b688854a15966e1736bf5f17bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 18\ttrain_loss: 3.34e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b40af02ce1b42248b8e8bf816d5ae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 19\ttrain_loss: 3.25e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d43d70ca9c4fd5870c45ac42a829aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=15, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch # 20\ttrain_loss: 3.17e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam([p for p in myModel.parameters() if p.requires_grad], lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "myModel.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = total = 0\n",
    "    progress_bar = tqdm_notebook(train_loader, desc='Training')\n",
    "    \n",
    "    for inputs, targets, lengths in progress_bar:\n",
    "        # Clean old gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forwards pass\n",
    "        loss = myModel(inputs, targets, lengths)\n",
    "\n",
    "        # Perform gradient descent, backwards pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Take a step in the right direction\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record metrics\n",
    "        total_loss += loss.item()\n",
    "        total += targets.size(1)\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    \n",
    "    tqdm.write(f'epoch #{epoch + 1:3d}\\ttrain_loss: {train_loss:.2e}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.34958690404892\n"
     ]
    }
   ],
   "source": [
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(myModel.state_dict(), os.path.join(os.getcwd(),\"models\",\"Seq2SeqForEngHin-attempt1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EngToHinModel(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(3000, 100)\n",
       "    (gru): GRU(100, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3000, 100)\n",
       "    (gru): GRU(228, 128, batch_first=True)\n",
       "    (fc): Linear(in_features=128, out_features=3000, bias=True)\n",
       "    (W1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (V): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.load_state_dict(torch.load(os.path.join(os.getcwd(),\"models\",\"Seq2SeqForEngHin-attempt1.pt\")))\n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
