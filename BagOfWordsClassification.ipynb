{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "I will be building a simple BagOfWords text classifier using PyTorch. It will be trained on the IMDB movie reviews dataset. So, i start with importing necessary libraries and defining key variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PROJECTS\\Github\\nlp-basics cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR,\"data\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(ROOT_DIR,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  Once again Mr. Costner has dragged out a movie...      0\n",
       "1  This is an example of why the majority of acti...      0\n",
       "2  First of all I hate those moronic rappers, who...      0\n",
       "3  Not even the Beatles could write songs everyon...      0\n",
       "4  Brass pictures (movies is not a fitting word f...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR,\"imdb_reviews.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "We are provided with a dataset with 62155 textual reviews, each having a positive(1) or a negative(0) label. There are no NaNs so it is safe to proceed. There are 31285 Negative and 30870 positive reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Representation\n",
    "\n",
    "So, we have our dataset loaded but all the data is in text format. To convert this to a quantitative \"Tensor\", we need a way or some kind of \"representation\". An easy way of representing the text data in terms of numbers is to:\n",
    "\n",
    "1. Split the text into individual words\n",
    "2. Compare it against a list of known words\n",
    "3. Count the number of times a particular word occurs\n",
    "\n",
    "This is the basic idea behind BoW, we split texts into words, compare it against our \"Vocabulary\" or set of words and finally find counts of each known word. So, if our Vocabulary is of 5 words say, \n",
    "\n",
    "{\"alpha\",\"dog\",\"jumps\",\"quick\",\"the\"}\n",
    "\n",
    "then the sentence:\n",
    "\"The quick brown fox jumps over the lazy dog\"\n",
    "should generate the following 5x1 vector (as our vocab is size 5)\n",
    "\n",
    "1. [\"the\" , \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]  (also called tokenization)\n",
    "2. { 0 alphas , 1 dog , 1 jumps , 1 quick , 2 the } (also called numericalization)\n",
    "3. thus, input vector for sentence = [ 0 , 1 , 1 , 1 , 2 ]\n",
    "\n",
    "This is a basic rundown version. Optimizations to this changes model behaviour and accuracy by significant factor. It should also be noted that the vocab size directly affects the memory needed as each input row will have number of features equal to our original vocab size. Thus, the vocabulary size needs to be as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our own Datasets\n",
    "\n",
    "We define a class that holds our inputs and targets as class variables. We also need to override \"\\__getitem__()\" and \"\\__len__()\" to return the i-th item and the length of our dataset respectively.\n",
    "\n",
    "Then we define a DataLoader object that takes a dataset and a batchsize to load objects into memory. As seen from below print statements, getting the 6-th item from the dataset returns a tuple (X,y) where X would be a feature vector and y would be a target vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Sequences(Dataset):\n",
    "    def __init__(self,path):\n",
    "        \"\"\"\n",
    "            Create a dataset by reading a CSV from \"path\"\n",
    "            Args:\n",
    "            path (Path or os.path) : the location of the CSV file to be read. Suffix is to be added.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            self.vector = CountVectorizer(stop_words='english',max_df=0.99,min_df=0.005)\n",
    "            self.seq = self.vector.fit_transform(df.review.tolist())\n",
    "            self.labels = df.label.tolist()\n",
    "            self.token2idx = self.vector.vocabulary_\n",
    "            self.idx2token = {idx:token for token,idx in self.token2idx.items()}\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"FileNotFoundError: {path} doesn't specify a CSV file.\\n Please check the pathname again\\n\")\n",
    "            print(\"Cannot create dataset!\")\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        \"\"\"\n",
    "            Gets the i-th input and its label in the dataset\n",
    "            i (int): the index number\n",
    "        \"\"\"        \n",
    "        return self.seq[i,:].toarray(),self.labels[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            Allows the use of len() function on our object\n",
    "        \"\"\"\n",
    "        return self.seq.shape[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ds = Sequences(os.path.join(DATA_DIR,\"imdb_reviews.csv\"))\n",
    "train_dl = DataLoader(ds,batch_size=4096)\n",
    "print(ds[5][0][:20])\n",
    "print(ds[5][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the matrices\n",
    "\n",
    "The dataset has stored the CSV data into \"self.seq\" which is our input matrix and \"self.labels\" which is our target matrix. The input matrix has a size of (62155 , 3028). Meaning that we have 62155 reviews and our vocabulary size is 3028."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62155, 3028)\n",
      "62155\n"
     ]
    }
   ],
   "source": [
    "print(ds.seq.shape)\n",
    "print(len(ds.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "With our dataloader setup, we now define our architecture. The first model would be a simple one to see if everything works or not. The model would look like:\n",
    "\n",
    "Layer 1 : x1 = W1.X + b1\n",
    "\n",
    "Layer 2 : h1 = ReLU(x1)\n",
    "\n",
    "Layer 3 : x2 = W2.h + b2\n",
    "\n",
    "Layer 4 : o = sigmoid(x2)\n",
    "\n",
    "This will give us the probability of our review being positive or negative. To enhance our model (or specifically , W1,b1,W2,b2 ) , we define a loss function and update it by using an optimizer. We will be using NLL (Negative Log Likelihood) loss and Adam optimizer.\n",
    "\n",
    "We start with importing all the necessary libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"f\" in functional is lower-case although imported as upper case \"F\"\n",
    "# Keep this in mind to save yourself some \"moduleNotFound\" headache.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self,vocabSize,hiddenSizes):\n",
    "        \"\"\"\n",
    "            Define the model architecture by overriding nn.Module method\n",
    "            Remember to call the super function first \n",
    "            else everything goes haywire\n",
    "        \"\"\"\n",
    "    \n",
    "        super(BoWClassifier,self).__init__()\n",
    "        self.layer1 = nn.Linear(vocabSize,hiddenSizes[0])\n",
    "        self.layer2 = nn.Linear(hiddenSizes[0],hiddenSizes[1])\n",
    "        self.layer3 = nn.Linear(hiddenSizes[1],1)\n",
    "        \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "            Defines how forward prop occurs\n",
    "            Overriden through nn.Module\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass inputs to successive layers...\n",
    "        x = F.relu(self.layer1(inputs.squeeze(1).float()))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        \n",
    "        # And return final output...\n",
    "        return self.layer3(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifier(\n",
       "  (layer1): Linear(in_features=3028, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# myModel is back from previous repositories!...\n",
    "myModel = BoWClassifier(vocabSize=len(ds.token2idx),hiddenSizes=[128,64])\n",
    "myModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss...\n",
    "lossFunc = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# An optimizer computes gradients for parameters given to it...\n",
    "optimizer = optim.Adam([p for p in myModel.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This part deals with training our model. As our architecture is setup with a loss function and an optimizer, we move forward and let PyTorch handle the rest... \n",
    "\n",
    "Training has the following steps:\n",
    "\n",
    "For every epoch,\n",
    "    1. Get a batch of data through a dataloader (in the form (X,Y))\n",
    "    2. For every data row:\n",
    "        a. Zero out the gradients\n",
    "        b. Find output for the given input\n",
    "        c. Find the loss b/w output and target\n",
    "        d. Calculate gradients(also called Backward or backprop)\n",
    "        e. Update Parameters\n",
    "        f. Store the loss onto an array\n",
    "    3. Store the total batch-wise loss into another list\n",
    "    4. Print out the loss at end of each batch\n",
    "\n",
    "To do some of the above printing, we use \"tqdm\". It shows a progress bar and can display a few other things (It looks good!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\tTrain Loss : 0.273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2\tTrain Loss : 0.266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3\tTrain Loss : 0.260\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4\tTrain Loss : 0.256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5\tTrain Loss : 0.252\n"
     ]
    }
   ],
   "source": [
    "#tqdm is used to show a progress bar (eye-candy)...\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "\n",
    "myModel.train()\n",
    "trainLosses = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    progressBar = tqdm_notebook(train_dl,leave=False)\n",
    "    losses = []\n",
    "    n = 0\n",
    "    \n",
    "    for inputs,target in progressBar:\n",
    "        # Zero out all gradients...\n",
    "        myModel.zero_grad()\n",
    "        \n",
    "        # Calculate the current targets...\n",
    "        output = myModel(inputs)\n",
    "        \n",
    "        #Calculate losses...\n",
    "        loss = lossFunc(output.squeeze(),target.float())\n",
    "        \n",
    "        \n",
    "        #backward prop...\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(myModel.parameters(), 3)\n",
    "        \n",
    "        #update parameters...\n",
    "        optimizer.step()\n",
    "        \n",
    "        #setting up some eye-candy!...\n",
    "        progressBar.set_description(f\"Loss: {loss.item():.3f}\")\n",
    "        \n",
    "        #store it in a list...\n",
    "        losses.append(loss.item())\n",
    "        n+=1\n",
    "    \n",
    "    batchLoss = sum(losses)/n\n",
    "    trainLosses.append(batchLoss)\n",
    "    \n",
    "    tqdm.write(f\"Epoch : {epoch+1}\\tTrain Loss : {batchLoss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predciting stuff\n",
    "\n",
    "The models trained and now coming to the part where we use the model to predict if a given blob of text is positive or negative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    myModel.eval()\n",
    "    with torch.no_grad():\n",
    "        # Apply all the required transforms onto the text...\n",
    "        ip = torch.LongTensor(ds.vector.transform([text]).toarray())\n",
    "        \n",
    "        #forward prop on the input...\n",
    "        op = myModel(ip)\n",
    "        pred = torch.sigmoid(op).item()\n",
    "        \n",
    "        if pred > 0.5:\n",
    "            print(f\"{pred:.3f} : Positive\")\n",
    "        else:\n",
    "            print(f\"{pred:.3f} : Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets grab a few reviews...\n",
    "\n",
    "# First, the reeeallllyyy loooonnngggg IRISHMAN...\n",
    "# It was seriously too long...\n",
    "\n",
    "irishman = \"\"\"Martin Scorsese’s THE IRISHMAN is meant to be the director’s last word on the gangster film, being a genre he’s the undisputed master of.\n",
    "While MEAN STREETS, GOODFELLAS and CASINO emphasized the excitement and rock n’ roll aspect of the lifestyle, before the inevitable fall from grace, THE IRISHMAN tackles the real, human cost of such a lifestyle.\n",
    "Your only outs are: wind up dead or rot into old age with no one around to care about you, and that’s provided you somehow avoid prison.\n",
    "It’s a melancholy fate, and appropriately, so is the film, being perhaps more in the vein of RAGING BULL than GOODFELLAS or CASINO. \n",
    "It’s a contemplative epic, but also among the most vital films in recent memory. \n",
    "If anyone deserves to have the last word on gangsterdom, it’s Scorsese.\n",
    "It probably could have only ever been made by Netflix, with them giving him a budget somewhere in the neighborhood of $160 million.\n",
    "No traditional studio would ever finance a character-driven drama in such a way, much less allow him to put out a version that runs 3.5 hours. \n",
    "While lengthy, every frame of THE IRISHMAN is packed to the gills with substance. \n",
    "There’s not a moment when it drags, and it’s probably the fastest 3.5 hours you’re likely to ever spend on a film.\n",
    "The price tag, of course, can be attributed up to the CGI de-aging effects. \n",
    "For the film to work, De Niro has to be able to convincingly play a man from his late thirties to middle age. \n",
    "While yes, he never really looks anything less than middle-aged, you honestly forget all about the CGI after fifteen minutes. \n",
    "Rather, you get sucked into the story regardless of the effects. \n",
    "For those wondering why they took so long to make it, I can only point towards the last half hour of the film, which delivers a gut punch meditation on aging, would have been impossible to convey had De Niro himself not been approaching Sheehan’s on-screen age.\n",
    "While De Niro is in virtually every frame of the film, the supporting cast is exceptional, even by Scorsese’s standards. \n",
    "In some ways, it’s old home week, with Joe Pesci re-emerging from retirement to play Bufalino, while Harvey Keitel appears, as do newer Scorsese regulars (via his HBO shows) Stephen Graham, Domenick Lombardozzi, Bobby Cannavale, Ray Romano and Jack Huston (playing Robert Kennedy) put in memorable turns. \n",
    "Pesci hasn’t lost a beat despite largely being absent from the screen, with Bufalino a change of pace from his iconically live wire parts in GOODFELLAS and CASINO. \n",
    "Bufalino is a quieter sort of person, one who’s not quick to anger and even something of a peacemaker, even if he’s inevitably the deadliest of enemies.\n",
    "Of everyone though, the best role is no doubt, Jimmy Hoffa, with Al Pacino adding another one to his pantheon of great portrayals, sinking his teeth into the part like he hasn’t in years. \n",
    "Much of it relies on his banter with De Niro, and truly they are a great pair. \n",
    "Pacino gives the film it’s warmth, especially through his unexpectedly touching friendship with Sheehan’s daughter, Peggy (played as an adult by Anna Paquin), who’s terrified of her father and his cronies, but falls for this charismatic gent in a big way.\n",
    "Despite the more melancholy tone, THE IRISHMAN, like all of Scorsese’s films, is also often hilarious, from the way it depicts the minutia of mob life (everyone dumps their guns in the same place), to the absurdity of names and more. \n",
    "One of the better recurring motifs is how every time a gangster is introduced, they reveal the man’s ultimate fate, which involve a gruesome death or prison. \n",
    "In another departure, Scorsese emphasizes Robbie Robertson’s score over period tunes, with a few notable exceptions, including repeated, haunting use of The Five Satin’s “In the Still of the Night.”\n",
    "Truly, THE IRISHMAN feels like it could be the perfect capper to Scorsese/De Niro/Pacino and Pesci’s careers, although everyone is so perfect here I wouldn’t be surprised if they’re not lured into one last project together. \n",
    "If not though, no one could have gone out on a better film. \n",
    "THE IRISHMAN, or as it’s called on-screen, I HEARD YOU PAINT HOUSES, is a legitimate masterpiece.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 : Positive\n"
     ]
    }
   ],
   "source": [
    "predict(irishman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was....well, kinda weird but okay..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think its cheating but i couldnt think of anything worse than\n",
    "# GoT S8E3 (given its history) so here goes...\n",
    "# This review was taken from metacritic and is uncensored...\n",
    "#read at your own risk...\n",
    "\n",
    "got = \"\"\"They stole Jon Snow's destiny in this episode. \n",
    "Jon was always written to fight against the white walkers. \n",
    "To have someone else steal the kill shot from him, just to fit in with 2019 political agenda is disgusting. \n",
    "I've been waiting for this for over 20 years. \n",
    "This is not what was promised. \n",
    "The battle scene was completely stupid as well. \n",
    "Walls are meant to be hidden behind or mounted upon. \n",
    "Hopefully they haven't neutered GRRM as they did D&D. \n",
    "Ever since the show runners took over the story this show has been total crap.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.144 : Negative\n"
     ]
    }
   ],
   "source": [
    "predict(got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It said GoT S8E3 was terrible...its working :')\n",
    "\n",
    "Time to save this model :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    os.mkdir(os.path.join(ROOT_DIR,\"models\"),)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "SAVE_DIR = os.path.join(ROOT_DIR,\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(myModel.state_dict(), os.path.join(SAVE_DIR,\"BoWClassifier-128,64-0.25trainLoss.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifier(\n",
       "  (layer1): Linear(in_features=3028, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_FILE = os.path.join(SAVE_DIR,\"BoWClassifier-128,64-0.25trainLoss.pt\")\n",
    "\n",
    "myModel = BoWClassifier(vocabSize=len(ds.token2idx),hiddenSizes=[128,64])\n",
    "myModel.load_state_dict(torch.load(SAVE_FILE))\n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
